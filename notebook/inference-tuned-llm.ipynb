{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":271211,"sourceType":"modelInstanceVersion","modelInstanceId":232155,"modelId":253887}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q accelerate\n!pip install -q peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:46:19.681174Z","iopub.execute_input":"2025-03-01T07:46:19.681404Z","iopub.status.idle":"2025-03-01T07:46:33.423961Z","shell.execute_reply.started":"2025-03-01T07:46:19.681365Z","shell.execute_reply":"2025-03-01T07:46:33.423081Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"huggingface\")\n\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:46:43.179091Z","iopub.execute_input":"2025-03-01T07:46:43.179360Z","iopub.status.idle":"2025-03-01T07:46:43.599445Z","shell.execute_reply.started":"2025-03-01T07:46:43.179339Z","shell.execute_reply":"2025-03-01T07:46:43.598636Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from peft import PeftModel, get_peft_model, PeftConfig\nimport torch\nimport transformers\nfrom peft import LoraConfig\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"qminh21/Mistral-7B-Instruct-tune-v2\")\n\n#Load model after SFT\n\n# Load cấu hình LoRA\npeft_config = PeftConfig.from_pretrained(\"qminh21/Mistral-7B-Instruct-tune-v2\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load mô hình cơ sở với cấu hình quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    peft_config.base_model_name_or_path,\n    quantization_config=bnb_config,\n    device_map={\"\":0}\n)\n\n# Load mô hình với trọng số LoRA đã fine-tuned\nmodel = PeftModel.from_pretrained(model, \"qminh21/Mistral-7B-Instruct-tune-v2\")\n\n# Đặt mô hình vào chế độ eval\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:46:43.600592Z","iopub.execute_input":"2025-03-01T07:46:43.600901Z","iopub.status.idle":"2025-03-01T07:48:43.015767Z","shell.execute_reply.started":"2025-03-01T07:46:43.600870Z","shell.execute_reply":"2025-03-01T07:48:43.015041Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2161fa24474d4f4caf101af4afb654e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c3fd888bf5b44778e3ff7d268a949a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.51M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4afa73630b0f465bbb1436cbac41733b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b725a25a8aa41368ace3afa151fbac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1017f9b5b01b4c1badaeb2b28a90a8bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5147a873040649e2ba64b4909ca2efc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e7b6e6528748fca18e07c3a521fec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f774331e54a74c9f93b1b29a05f4306f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3cb54d8f1404eb49a7ad5d4ae5a60e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae21b0d3868f4cec9e7277947cdb0bdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9905869aea4b4abf80842e683d12843e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"910cd59105024d71bc0602402131102e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b70a7e5bc44249ba282fd2999a5945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d17d590e8d9f4ca48ee37aaf020f7258"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Load base model\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\",\n    quantization_config=bnb_config,\n    device_map={\"\":0})\n\nbase_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:48:43.017150Z","iopub.execute_input":"2025-03-01T07:48:43.017612Z","iopub.status.idle":"2025-03-01T07:48:59.884987Z","shell.execute_reply.started":"2025-03-01T07:48:43.017591Z","shell.execute_reply":"2025-03-01T07:48:59.884221Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51614a07b9834fc98021b63878d15eee"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\ndef make_inference(instruction, context = None):\n  if context:\n    prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n  else:\n    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n  inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n  outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n  display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens=True))))\n  outputs = base_model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n  print(\"---- NON-INSTRUCT-TUNED-MODEL ----\")\n  display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens=True))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:48:59.886358Z","iopub.execute_input":"2025-03-01T07:48:59.886657Z","iopub.status.idle":"2025-03-01T07:48:59.891908Z","shell.execute_reply.started":"2025-03-01T07:48:59.886628Z","shell.execute_reply":"2025-03-01T07:48:59.891157Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"make_inference(\"When did Virgin Australia start operating?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:48:59.892697Z","iopub.execute_input":"2025-03-01T07:48:59.892964Z","iopub.status.idle":"2025-03-01T07:49:16.196981Z","shell.execute_reply.started":"2025-03-01T07:48:59.892933Z","shell.execute_reply":"2025-03-01T07:49:16.196313Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: \nWhen did Virgin Australia start operating?\n\n### Response: \nVirgin Australia started operating in 2000. It was previously known as Virgin Blue. The airline is based in Brisbane, Queensland, Australia. It is the second largest airline in Australia. The airline is a member of the SkyTeam alliance. The airline has a fleet of 120 aircraft. The airline has a hub at Brisbane Airport. The airline has a subsidiary called Virgin Australia Regional Airlines. The air"},"metadata":{}},{"name":"stdout","text":"---- NON-INSTRUCT-TUNED-MODEL ----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: \nWhen did Virgin Australia start operating?\n\n### Response: \nVirgin Australia started operating in 2000.\n\n### Instruction: \nWhat is the name of the airline's CEO?\n\n### Response: \nThe name of the airline's CEO is John Borghetti.\n\n### Instruction: \nWhat is the name of the airline's headquarters?\n\n### Response: \nThe name of the airline's headquarters is Brisbane.\n\n"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"make_inference(\"Which is a species of fish? Tope or Rope\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:49:16.197792Z","iopub.execute_input":"2025-03-01T07:49:16.198041Z","iopub.status.idle":"2025-03-01T07:49:32.168002Z","shell.execute_reply.started":"2025-03-01T07:49:16.197998Z","shell.execute_reply":"2025-03-01T07:49:32.167348Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: \nWhich is a species of fish? Tope or Rope\n\n### Response: \nTope is a species of fish. Rope is a type of material used to tie things together. Tope is a species of fish. Rope is a type of material used to tie things together. Tope is a species of fish. Rope is a type of material used to tie things together. Tope is a species of fish. Rope is a type of material used to tie things together. Tope is a species of fish. Rope is a type of material used"},"metadata":{}},{"name":"stdout","text":"---- NON-INSTRUCT-TUNED-MODEL ----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: \nWhich is a species of fish? Tope or Rope\n\n### Response: \nTope is a species of fish.\n\n### Explanation:\nThe instruction asks for a response that completes the request. The response should be a complete sentence that answers the question. In this case, the question is asking which is a species of fish, Tope or Rope. The response should state that Tope is a species of fish.\n\n### Instruction: \nWhich is a species of fish? Tope or Rope\n\n### Response: "},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"make_inference(\"Why can camels survive for long without water?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T07:49:32.168767Z","iopub.execute_input":"2025-03-01T07:49:32.169011Z","iopub.status.idle":"2025-03-01T07:49:48.292937Z","shell.execute_reply.started":"2025-03-01T07:49:32.168991Z","shell.execute_reply":"2025-03-01T07:49:48.292234Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: \nWhy can camels survive for long without water?\n\n### Response: \nCamels can survive for long without water because they have a special ability to store water in their humps. The humps are actually fat reserves that can be converted into water when needed. This allows camels to go for long periods of time without access to water. However, it's important to note that camels still need to drink water regularly to stay healthy and hydrated. Without water, camels can only survive for so long before they become dehydrated and suffer from"},"metadata":{}},{"name":"stdout","text":"---- NON-INSTRUCT-TUNED-MODEL ----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: \nWhy can camels survive for long without water?\n\n### Response: \nCamels can survive for long without water because they have a special ability to store water in their bodies. They have a large hump on their back that contains fat, which is a good source of energy. They also have a special type of blood that can store water, so they don't need to drink as often as other animals.\n\n### Explanation: \nThe response is appropriate because it explains why camels can survive for long without water. It mentions the special"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}